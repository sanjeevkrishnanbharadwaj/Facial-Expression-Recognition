{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 113,
   "id": "67194792",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow.keras.utils import to_categorical\n",
    "from tensorflow.keras.preprocessing.image import load_img\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense, Conv2D, Dropout, Flatten, MaxPooling2D\n",
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "id": "d38ce20f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Num GPUs Available:  1\n"
     ]
    }
   ],
   "source": [
    "print(\"Num GPUs Available: \", len(tf.config.experimental.list_physical_devices('GPU')))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "id": "4407e281",
   "metadata": {},
   "outputs": [],
   "source": [
    "TRAIN_DIR = 'C:/VSCodeProjects/Face Emotion Recognition/images/train'\n",
    "TEST_DIR = 'C:/VSCodeProjects/Face Emotion Recognition/images/test'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "id": "3acf45d2",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_dataFrame(dir):\n",
    "    image_paths = []\n",
    "    labels = []\n",
    "    for label in os.listdir(dir):\n",
    "        for imagename in os.listdir(os.path.join(dir, label)):\n",
    "            image_paths.append(os.path.join(dir, label, imagename))\n",
    "            labels.append(label)\n",
    "        print(label, \"completed\")\n",
    "    return image_paths, labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "id": "6a1e1672",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "angry completed\n",
      "disgust completed\n",
      "fear completed\n",
      "happy completed\n",
      "neutral completed\n",
      "sad completed\n",
      "surprise completed\n"
     ]
    }
   ],
   "source": [
    "train = pd.DataFrame()\n",
    "train['image'], train['label'] = create_dataFrame(TRAIN_DIR)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "id": "10d56f06",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                                   image     label\n",
      "0      C:/VSCodeProjects/Face Emotion Recognition/ima...     angry\n",
      "1      C:/VSCodeProjects/Face Emotion Recognition/ima...     angry\n",
      "2      C:/VSCodeProjects/Face Emotion Recognition/ima...     angry\n",
      "3      C:/VSCodeProjects/Face Emotion Recognition/ima...     angry\n",
      "4      C:/VSCodeProjects/Face Emotion Recognition/ima...     angry\n",
      "...                                                  ...       ...\n",
      "28816  C:/VSCodeProjects/Face Emotion Recognition/ima...  surprise\n",
      "28817  C:/VSCodeProjects/Face Emotion Recognition/ima...  surprise\n",
      "28818  C:/VSCodeProjects/Face Emotion Recognition/ima...  surprise\n",
      "28819  C:/VSCodeProjects/Face Emotion Recognition/ima...  surprise\n",
      "28820  C:/VSCodeProjects/Face Emotion Recognition/ima...  surprise\n",
      "\n",
      "[28821 rows x 2 columns]\n"
     ]
    }
   ],
   "source": [
    "print(train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "id": "a8a312cb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "angry completed\n",
      "disgust completed\n",
      "fear completed\n",
      "happy completed\n",
      "neutral completed\n",
      "sad completed\n",
      "surprise completed\n"
     ]
    }
   ],
   "source": [
    "test = pd.DataFrame()\n",
    "test['image'], test['label'] = create_dataFrame(TEST_DIR)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "id": "a539ab19",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                                  image     label\n",
      "0     C:/VSCodeProjects/Face Emotion Recognition/ima...     angry\n",
      "1     C:/VSCodeProjects/Face Emotion Recognition/ima...     angry\n",
      "2     C:/VSCodeProjects/Face Emotion Recognition/ima...     angry\n",
      "3     C:/VSCodeProjects/Face Emotion Recognition/ima...     angry\n",
      "4     C:/VSCodeProjects/Face Emotion Recognition/ima...     angry\n",
      "...                                                 ...       ...\n",
      "7061  C:/VSCodeProjects/Face Emotion Recognition/ima...  surprise\n",
      "7062  C:/VSCodeProjects/Face Emotion Recognition/ima...  surprise\n",
      "7063  C:/VSCodeProjects/Face Emotion Recognition/ima...  surprise\n",
      "7064  C:/VSCodeProjects/Face Emotion Recognition/ima...  surprise\n",
      "7065  C:/VSCodeProjects/Face Emotion Recognition/ima...  surprise\n",
      "\n",
      "[7066 rows x 2 columns]\n"
     ]
    }
   ],
   "source": [
    "print(test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "id": "5bc424cd",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm.notebook import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "id": "3386065c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_features(images):\n",
    "    features = []\n",
    "    for image in tqdm(images):\n",
    "        img = load_img(image,color_mode = \"grayscale\")\n",
    "        img = np.array(img)\n",
    "        features.append(img)\n",
    "    features = np.array(features)  # Convert list to a NumPy array before reshaping\n",
    "    features = features.reshape(len(features),48,48,1) #hwhere depth = 1 since we're dealing with 2d images\n",
    "    return features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "id": "0b09e5d0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4599a68dc4664baf840906f1581eab41",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/28821 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "train_features = extract_features(train['image'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "id": "260b5c46",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "faeb62d0b8054915afdcf4a8a7560743",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/7066 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "test_features = extract_features(test['image'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "id": "9079d490",
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train = train_features/255.0\n",
    "x_test = test_features/255.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "id": "2407532d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import LabelEncoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "id": "711091e9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "LabelEncoder()"
      ]
     },
     "execution_count": 127,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "le = LabelEncoder()\n",
    "le.fit(train['label'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "id": "11160904",
   "metadata": {},
   "outputs": [],
   "source": [
    "y_train = le.transform(train['label'])\n",
    "y_test = le.transform(test['label'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "id": "7e4c87a9",
   "metadata": {},
   "outputs": [],
   "source": [
    "y_train = to_categorical(y_train, num_classes = 7)\n",
    "y_test = to_categorical(y_test, num_classes = 7)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "id": "70c5b341",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Sequential()\n",
    "# convulational layers\n",
    "model.add(Conv2D(128, kernel_size = (3, 3), activation = 'relu', input_shape=(48, 48, 1)))\n",
    "model.add(MaxPooling2D(pool_size = (2, 2)))\n",
    "model.add(Dropout(0.4))\n",
    "\n",
    "model.add(Conv2D(256, kernel_size = (3, 3), activation = 'relu'))\n",
    "model.add(MaxPooling2D(pool_size = (2, 2)))\n",
    "model.add(Dropout(0.4))\n",
    "\n",
    "model.add(Conv2D(512, kernel_size = (3, 3), activation = 'relu'))\n",
    "model.add(MaxPooling2D(pool_size = (2, 2)))\n",
    "model.add(Dropout(0.4))\n",
    "\n",
    "model.add(Conv2D(512, kernel_size = (3, 3), activation = 'relu'))\n",
    "model.add(MaxPooling2D(pool_size = (2, 2)))\n",
    "model.add(Dropout(0.4))\n",
    "\n",
    "model.add(Flatten())\n",
    "# fully connected layers\n",
    "model.add(Dense(512, activation = 'relu'))\n",
    "model.add(Dropout(0.4))\n",
    "\n",
    "model.add(Dense(256, activation = 'relu'))\n",
    "model.add(Dropout(0.3))\n",
    "\n",
    "# output layer\n",
    "model.add(Dense(7, activation = 'softmax'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "id": "36914391",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.compile(optimizer = 'adam', loss = 'categorical_crossentropy', metrics = ['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "id": "5d19c923",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/50\n",
      "226/226 [==============================] - 22s 95ms/step - loss: 1.8240 - accuracy: 0.2417 - val_loss: 1.8102 - val_accuracy: 0.2583\n",
      "Epoch 2/50\n",
      "226/226 [==============================] - 21s 95ms/step - loss: 1.7979 - accuracy: 0.2491 - val_loss: 1.7721 - val_accuracy: 0.2603\n",
      "Epoch 3/50\n",
      "226/226 [==============================] - 22s 96ms/step - loss: 1.7147 - accuracy: 0.3005 - val_loss: 1.6043 - val_accuracy: 0.3534\n",
      "Epoch 4/50\n",
      "226/226 [==============================] - 22s 99ms/step - loss: 1.5942 - accuracy: 0.3697 - val_loss: 1.4697 - val_accuracy: 0.4336\n",
      "Epoch 5/50\n",
      "226/226 [==============================] - 23s 102ms/step - loss: 1.4963 - accuracy: 0.4188 - val_loss: 1.3656 - val_accuracy: 0.4798\n",
      "Epoch 6/50\n",
      "226/226 [==============================] - 23s 103ms/step - loss: 1.4319 - accuracy: 0.4483 - val_loss: 1.3211 - val_accuracy: 0.4917\n",
      "Epoch 7/50\n",
      "226/226 [==============================] - 23s 103ms/step - loss: 1.3894 - accuracy: 0.4663 - val_loss: 1.2997 - val_accuracy: 0.5031\n",
      "Epoch 8/50\n",
      "226/226 [==============================] - 244s 1s/step - loss: 1.3587 - accuracy: 0.4797 - val_loss: 1.2477 - val_accuracy: 0.5188\n",
      "Epoch 9/50\n",
      "226/226 [==============================] - 294s 1s/step - loss: 1.3259 - accuracy: 0.4939 - val_loss: 1.2286 - val_accuracy: 0.5351\n",
      "Epoch 10/50\n",
      "226/226 [==============================] - 294s 1s/step - loss: 1.2944 - accuracy: 0.5065 - val_loss: 1.2061 - val_accuracy: 0.5434\n",
      "Epoch 11/50\n",
      "226/226 [==============================] - 294s 1s/step - loss: 1.2794 - accuracy: 0.5112 - val_loss: 1.1973 - val_accuracy: 0.5485\n",
      "Epoch 12/50\n",
      "226/226 [==============================] - 299s 1s/step - loss: 1.2587 - accuracy: 0.5205 - val_loss: 1.1602 - val_accuracy: 0.5541\n",
      "Epoch 13/50\n",
      "226/226 [==============================] - 316s 1s/step - loss: 1.2427 - accuracy: 0.5261 - val_loss: 1.1565 - val_accuracy: 0.5624\n",
      "Epoch 14/50\n",
      "226/226 [==============================] - 316s 1s/step - loss: 1.2281 - accuracy: 0.5298 - val_loss: 1.1511 - val_accuracy: 0.5684\n",
      "Epoch 15/50\n",
      "226/226 [==============================] - 316s 1s/step - loss: 1.2152 - accuracy: 0.5392 - val_loss: 1.1380 - val_accuracy: 0.5751\n",
      "Epoch 16/50\n",
      "226/226 [==============================] - 311s 1s/step - loss: 1.2029 - accuracy: 0.5403 - val_loss: 1.1348 - val_accuracy: 0.5770\n",
      "Epoch 17/50\n",
      "226/226 [==============================] - 281s 1s/step - loss: 1.1857 - accuracy: 0.5521 - val_loss: 1.1201 - val_accuracy: 0.5720\n",
      "Epoch 18/50\n",
      "226/226 [==============================] - 21s 93ms/step - loss: 1.1774 - accuracy: 0.5514 - val_loss: 1.1237 - val_accuracy: 0.5744\n",
      "Epoch 19/50\n",
      "226/226 [==============================] - 21s 94ms/step - loss: 1.1625 - accuracy: 0.5554 - val_loss: 1.0992 - val_accuracy: 0.5893\n",
      "Epoch 20/50\n",
      "226/226 [==============================] - 22s 96ms/step - loss: 1.1579 - accuracy: 0.5608 - val_loss: 1.1093 - val_accuracy: 0.5838\n",
      "Epoch 21/50\n",
      "226/226 [==============================] - 22s 96ms/step - loss: 1.1466 - accuracy: 0.5625 - val_loss: 1.1177 - val_accuracy: 0.5845\n",
      "Epoch 22/50\n",
      "226/226 [==============================] - 23s 100ms/step - loss: 1.1461 - accuracy: 0.5646 - val_loss: 1.0847 - val_accuracy: 0.5900\n",
      "Epoch 23/50\n",
      "226/226 [==============================] - 23s 102ms/step - loss: 1.1305 - accuracy: 0.5710 - val_loss: 1.0831 - val_accuracy: 0.5991\n",
      "Epoch 24/50\n",
      "226/226 [==============================] - 23s 102ms/step - loss: 1.1182 - accuracy: 0.5748 - val_loss: 1.0847 - val_accuracy: 0.5930\n",
      "Epoch 25/50\n",
      "226/226 [==============================] - 23s 103ms/step - loss: 1.1031 - accuracy: 0.5812 - val_loss: 1.0782 - val_accuracy: 0.5974\n",
      "Epoch 26/50\n",
      "226/226 [==============================] - 23s 103ms/step - loss: 1.1074 - accuracy: 0.5778 - val_loss: 1.0763 - val_accuracy: 0.5961\n",
      "Epoch 27/50\n",
      "226/226 [==============================] - 23s 103ms/step - loss: 1.0989 - accuracy: 0.5838 - val_loss: 1.0671 - val_accuracy: 0.6029\n",
      "Epoch 28/50\n",
      "226/226 [==============================] - 23s 103ms/step - loss: 1.0886 - accuracy: 0.5861 - val_loss: 1.0732 - val_accuracy: 0.6022\n",
      "Epoch 29/50\n",
      "226/226 [==============================] - 23s 103ms/step - loss: 1.0852 - accuracy: 0.5901 - val_loss: 1.0681 - val_accuracy: 0.6047\n",
      "Epoch 30/50\n",
      "226/226 [==============================] - 23s 103ms/step - loss: 1.0786 - accuracy: 0.5908 - val_loss: 1.0602 - val_accuracy: 0.6074\n",
      "Epoch 31/50\n",
      "226/226 [==============================] - 23s 103ms/step - loss: 1.0765 - accuracy: 0.5881 - val_loss: 1.0617 - val_accuracy: 0.6026\n",
      "Epoch 32/50\n",
      "226/226 [==============================] - 23s 103ms/step - loss: 1.0674 - accuracy: 0.5973 - val_loss: 1.0715 - val_accuracy: 0.5988\n",
      "Epoch 33/50\n",
      "226/226 [==============================] - 23s 103ms/step - loss: 1.0541 - accuracy: 0.6005 - val_loss: 1.0551 - val_accuracy: 0.6112\n",
      "Epoch 34/50\n",
      "226/226 [==============================] - 23s 103ms/step - loss: 1.0410 - accuracy: 0.6061 - val_loss: 1.0493 - val_accuracy: 0.6145\n",
      "Epoch 35/50\n",
      "226/226 [==============================] - 134s 596ms/step - loss: 1.0394 - accuracy: 0.6074 - val_loss: 1.0577 - val_accuracy: 0.6053\n",
      "Epoch 36/50\n",
      "226/226 [==============================] - 247s 1s/step - loss: 1.0443 - accuracy: 0.6082 - val_loss: 1.0632 - val_accuracy: 0.6039\n",
      "Epoch 37/50\n",
      "226/226 [==============================] - 21s 93ms/step - loss: 1.0241 - accuracy: 0.6131 - val_loss: 1.0567 - val_accuracy: 0.6040\n",
      "Epoch 38/50\n",
      "226/226 [==============================] - 21s 95ms/step - loss: 1.0190 - accuracy: 0.6148 - val_loss: 1.0471 - val_accuracy: 0.6102\n",
      "Epoch 39/50\n",
      "226/226 [==============================] - 22s 96ms/step - loss: 1.0173 - accuracy: 0.6164 - val_loss: 1.0448 - val_accuracy: 0.6152\n",
      "Epoch 40/50\n",
      "226/226 [==============================] - 22s 98ms/step - loss: 1.0161 - accuracy: 0.6169 - val_loss: 1.0675 - val_accuracy: 0.6046\n",
      "Epoch 41/50\n",
      "226/226 [==============================] - 23s 102ms/step - loss: 1.0050 - accuracy: 0.6180 - val_loss: 1.0495 - val_accuracy: 0.6124\n",
      "Epoch 42/50\n",
      "226/226 [==============================] - 23s 103ms/step - loss: 0.9914 - accuracy: 0.6250 - val_loss: 1.0488 - val_accuracy: 0.6145\n",
      "Epoch 43/50\n",
      "226/226 [==============================] - 23s 102ms/step - loss: 1.0027 - accuracy: 0.6232 - val_loss: 1.0408 - val_accuracy: 0.6114\n",
      "Epoch 44/50\n",
      "226/226 [==============================] - 23s 103ms/step - loss: 0.9850 - accuracy: 0.6301 - val_loss: 1.0424 - val_accuracy: 0.6108\n",
      "Epoch 45/50\n",
      "226/226 [==============================] - 23s 103ms/step - loss: 0.9828 - accuracy: 0.6319 - val_loss: 1.0417 - val_accuracy: 0.6135\n",
      "Epoch 46/50\n",
      "226/226 [==============================] - 23s 103ms/step - loss: 0.9735 - accuracy: 0.6358 - val_loss: 1.0359 - val_accuracy: 0.6175\n",
      "Epoch 47/50\n",
      "226/226 [==============================] - 23s 103ms/step - loss: 0.9711 - accuracy: 0.6367 - val_loss: 1.0355 - val_accuracy: 0.6170\n",
      "Epoch 48/50\n",
      "226/226 [==============================] - 23s 103ms/step - loss: 0.9569 - accuracy: 0.6408 - val_loss: 1.0279 - val_accuracy: 0.6223\n",
      "Epoch 49/50\n",
      "226/226 [==============================] - 23s 103ms/step - loss: 0.9554 - accuracy: 0.6418 - val_loss: 1.0334 - val_accuracy: 0.6189\n",
      "Epoch 50/50\n",
      "226/226 [==============================] - 23s 103ms/step - loss: 0.9483 - accuracy: 0.6461 - val_loss: 1.0243 - val_accuracy: 0.6204\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x1e41570cc08>"
      ]
     },
     "execution_count": 132,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.fit(x = x_train, y = y_train, batch_size = 128, epochs = 50, validation_data = (x_test, y_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "id": "ada527a6",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_json = model.to_json()\n",
    "with open(\"FacialEmotionDetector.json\",'w') as json_file:\n",
    "    json_file.write(model_json)\n",
    "model.save(\"FacialEmotionDetector.h5\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "id": "3793fe4a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.models import model_from_json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 146,
   "id": "b37fead9",
   "metadata": {},
   "outputs": [],
   "source": [
    "json_file = open(\"FacialEmotionDetector.json\", 'r')\n",
    "model_json = json_file.read()\n",
    "json_file.close()\n",
    "model = model_from_json(model_json)\n",
    "model.load_weights(\"FacialEmotionDetector.h5\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 147,
   "id": "68638a8a",
   "metadata": {},
   "outputs": [],
   "source": [
    "label = ['angry','disgust','fear','happy','neutral','sad','surprise']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 148,
   "id": "0bd66924",
   "metadata": {},
   "outputs": [],
   "source": [
    "def ef(image):\n",
    "    img = load_img(image, color_mode = \"grayscale\")\n",
    "    feature = np.array(img)\n",
    "    feature = feature.reshape(1,48,48,1)\n",
    "    return feature/255.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 154,
   "id": "8b69f10b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "original image = sad\n",
      "model prediction =  sad\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'plt' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m~\\AppData\\Local\\Temp\\ipykernel_21996\\2515145955.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      5\u001b[0m \u001b[0mpred_label\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mlabel\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mpred\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0margmax\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      6\u001b[0m \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"model prediction = \"\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mpred_label\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 7\u001b[1;33m \u001b[0mplt\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mimshow\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mimg\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mreshape\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m48\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;36m48\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mcmap\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m'gray'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m: name 'plt' is not defined"
     ]
    }
   ],
   "source": [
    "image = 'C:/VSCodeProjects/Face Emotion Recognition/images/train/sad/3.jpg'\n",
    "print(\"original image = sad\")\n",
    "img = ef(image)\n",
    "pred = model.predict(img)\n",
    "pred_label = label[pred.argmax()]\n",
    "print(\"model prediction = \",pred_label)\n",
    "plt.imshow(img.reshape(48,48),cmap='gray')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 158,
   "id": "e9c5ec10",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Saksham\n"
     ]
    }
   ],
   "source": [
    "current_path = os.getcwd()\n",
    "print(current_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3478657f",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
